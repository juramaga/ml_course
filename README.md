# Machine Learning Course

Machine Learning Course, Columbia 2019

## Format 

4 days of lectures, approximately one per morning and one per afternoon. Intermittent breakout work by participants.

## Lecture Outline 

### Introductions & Course Goals

* (Lecture 1) Computational and Inferential Thinking
   * machine learning & statistics introduction
   * Problem definition & Data
   * Exploration: visualization & data preparation
   * Featurization and Pipelining

### Representation Learning

* (Lecture 2) Supervised Learning (I)
   * Regression: logistic regression, kNN, Gaussian Processes
   * Classification: Random Forest & LightGBM

* (Lecture 3) Unsupervised Learning 
   * Clustering approaches
   * Anomaly detections with random forests

### Neutral Networks
* (Lecture 4) Neural Networks
   * Introductory algorithms and frameworks
   * Fully connected networks for regression
  
* (Lecture 5) Deep Convolutional Neural Networks
   * imaging classification
   * time-series classification
   * temporal convolution NN (TCNs)

* (Lecture 6) Generative and Compressive Modelling
   * auto-encoders for (semi- or unsupervised) learning
   * GANs
   * surrogate emulation

* (Lecture 7) Semi-supervised & Reinforcement Learning

### ML In the Real World

(Lecture 8) 

* Business considerations
* Deployability, Scaling, and Maintainability
* Bias, Reproducibility, GDPR, and Ethics in ML


## Frameworks:
  * Python 3.6 (or 3.7)
  * numpy, scipy, seaborn
  * sklearn
  * keras/tensorflow
 
 ## Commennts by RMG
I like the syllabus. Specially the last part on real world issues. Several people asked about ethical considerations last time, so I would like to see some about this. I also like the ordering of topics, which more or less matches what we did last time, with a bit more weight on NNs, which I think is fine. One thing I do worry about is the struggle between having as much content as we can (that is coherent), and the limited time. Some of the lectures seem heavy to me. For example, regression and classification in a single lecture (lecture 2) can be challenging. I say this based on the experience last year, but I am happy to be convienced otherwise.  Same is true for lecture 6. If we want them to properly grasp GANs, we might need a full lecture for it. Bottom line: I do like all that I see(and I wish we could fit it all), but I would prioritize depth over coverage if needed. We might have a clearer view once a more detailed plan for the lectures is laid. If we were to scarifice something, I would go for Reinforcement Learning, which I consider an advanced topic for this audience. That way we might be able to fit GANs.
